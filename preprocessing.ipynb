{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1f40EeRuvAkO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from opencv-python-headless) (1.26.1)\n",
      "frames [299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 300, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 300, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 300, 299, 299, 299, 300, 300, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 300, 300, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 300, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 300, 300, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 300, 300, 299, 300, 299, 299, 299, 299, 300, 299, 300, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 300, 299, 300, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 300, 300, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 300, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 300, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 300, 300, 299, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 300, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 300, 299, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 300, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 300, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 300, 299, 300, 299, 299, 299, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 299, 300, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 300, 300, 299, 300, 300, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 300, 299, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 300, 299, 299, 299, 300, 300, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 300, 299, 299, 300, 299, 300, 300, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 300, 300, 299, 299, 299, 300, 300, 299, 299, 300, 299, 299, 299, 299, 300, 299, 300, 299, 300, 300, 299, 299, 299, 299, 299, 299, 300, 299, 299, 299, 299, 299, 299, 299, 300, 299, 299, 300, 300, 299, 300, 299, 299, 299, 300, 300, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 299, 300, 300, 299, 299]\n",
      "Total number of videos: 1699\n",
      "Average frame per video: 299.1983519717481\n"
     ]
    }
   ],
   "source": [
    "# install openCV package\n",
    "!pip3 install opencv-python-headless\n",
    "\n",
    "# To get the average frame count\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Update the path to your desired directory\n",
    "video_directory = 'F:/Master/Research Project/Datasets/DFDC/Experiment Dataset/dfdc_train_part_1_exp'\n",
    "\n",
    "# To get the average frame count \n",
    "video_files =  glob.glob(video_directory + '/*.mp4')\n",
    "video_files = [os.path.normpath(file) for file in video_files]\n",
    "\n",
    "frame_count = []\n",
    "for video_file in video_files:\n",
    "  cap = cv2.VideoCapture(video_file)\n",
    "  if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) < 150):\n",
    "    video_files.remove(video_file)\n",
    "    continue\n",
    "  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "print(\"frames\", frame_count)\n",
    "print(\"Total number of videos:\", len(frame_count))\n",
    "print('Average frame per video:', np.mean(frame_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from facenet-pytorch) (1.26.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from facenet-pytorch) (2.31.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from facenet-pytorch) (0.16.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from facenet-pytorch) (10.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->facenet-pytorch) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->facenet-pytorch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->facenet-pytorch) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->facenet-pytorch) (2023.7.22)\n",
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torchvision->facenet-pytorch) (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==2.1.0->torchvision->facenet-pytorch) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==2.1.0->torchvision->facenet-pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==2.1.0->torchvision->facenet-pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch==2.1.0->torchvision->facenet-pytorch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch==2.1.0->torchvision->facenet-pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch==2.1.0->torchvision->facenet-pytorch) (1.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install facenet-pytorch\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to extract frame into images\n",
    "def frame_extract(path):\n",
    "    vidObj = cv2.VideoCapture(path)\n",
    "    success = 1\n",
    "    while success:\n",
    "        success, image = vidObj.read()\n",
    "        if success:\n",
    "            yield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# Determine if an nvidia GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on device: {}\".format(device))\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# create a face detection pipeline using MTCNN\n",
    "# mtcnn = MTCNN(keep_all=True, image_size=224, margin=0, device=device)\n",
    "mtcnn = MTCNN(keep_all=True, image_size=224, margin=0)\n",
    "\n",
    "# Create an inception resnet (in eval mode):\n",
    "resnet = InceptionResnetV1(pretrained=\"vggface2\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tracked = []\n",
    "aligned = []\n",
    "\n",
    "\n",
    "\n",
    "# to create face images\n",
    "def create_face_images(path_list, out_dir):\n",
    "    already_present_count = glob.glob(out_dir + \"*.jpg\")\n",
    "    print(\"No of images already present \", len(already_present_count))\n",
    "    # print(path_list)\n",
    "    # print(out_dir)\n",
    "\n",
    "    for path in tqdm(path_list):\n",
    "        out_path = os.path.join(out_dir, os.path.splitext(os.path.basename(path))[0])\n",
    "        file_exists = glob.glob(out_path + \"*.jpg\")\n",
    "        # print(file_exists)\n",
    "        if len(file_exists) != 0:\n",
    "            print(\"Files Already exist: \", out_path)\n",
    "            continue\n",
    "        frames = [\n",
    "            Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            for frame in frame_extract(path)\n",
    "        ]\n",
    "\n",
    "        for idx, frame in enumerate(frames):\n",
    "            print(frame)\n",
    "            if idx <= 150:\n",
    "                # Detect faces using MTCNN\n",
    "                boxes, probs = mtcnn.detect(frame)\n",
    "                print(boxes)\n",
    "                print(probs)\n",
    "                aligned.append(boxes)\n",
    "\n",
    "                # Draw faces\n",
    "                frame_draw = frame.copy()\n",
    "                draw = ImageDraw.Draw(frame_draw)\n",
    "\n",
    "                # Add to frame list\n",
    "                frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
    "\n",
    "                for i, face in enumerate(boxes):\n",
    "                    bbox = face\n",
    "                    # print(bbox)\n",
    "                    top, right, bottom, left = bbox\n",
    "                    print(top, right, bottom, left)\n",
    "\n",
    "                    draw.rectangle(bbox.tolist(), outline=(255, 0, 0), width=6)\n",
    "                    cropped_image = frame.crop(tuple(bbox))\n",
    "\n",
    "                    # Extract the face\n",
    "                    # face_image = frame[top:bottom, left:right, :]\n",
    "                    # face_image = frame[right : right + left, top : top + bottom]\n",
    "\n",
    "                    # Save the face as an image\n",
    "                    face_filename = f\"{out_path}_{idx}_face_{i}.jpg\"\n",
    "                    # cv2.imwrite(face_filename, face_image)\n",
    "                    cropped_image.save(face_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images already present  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1699 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDA80>\n",
      "[[518.234619140625 261.7532043457031 790.2434692382812 621.11572265625]\n",
      " [832.7639770507812 149.44363403320312 963.9954833984375\n",
      "  304.53179931640625]]\n",
      "[0.7906476855278015 0.979488730430603]\n",
      "518.234619140625 261.7532043457031 790.2434692382812 621.11572265625\n",
      "832.7639770507812 149.44363403320312 963.9954833984375 304.53179931640625\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE530>\n",
      "[[836.3551635742188 149.27163696289062 966.0438842773438\n",
      "  300.9661865234375]]\n",
      "[0.9832161068916321]\n",
      "836.3551635742188 149.27163696289062 966.0438842773438 300.9661865234375\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE410>\n",
      "[[835.569091796875 150.0800018310547 965.09130859375 303.7608337402344]]\n",
      "[0.9923816323280334]\n",
      "835.569091796875 150.0800018310547 965.09130859375 303.7608337402344\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDC60>\n",
      "[[831.8232421875 145.75489807128906 965.2982177734375 309.461181640625]]\n",
      "[0.9871649742126465]\n",
      "831.8232421875 145.75489807128906 965.2982177734375 309.461181640625\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE4D0>\n",
      "[[828.3588256835938 146.4226837158203 963.8223266601562 306.0989990234375]]\n",
      "[0.9930638670921326]\n",
      "828.3588256835938 146.4226837158203 963.8223266601562 306.0989990234375\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDBA0>\n",
      "[[828.5786743164062 148.42550659179688 961.9686279296875\n",
      "  307.6617431640625]]\n",
      "[0.9903809428215027]\n",
      "828.5786743164062 148.42550659179688 961.9686279296875 307.6617431640625\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDB10>\n",
      "[[828.868896484375 146.41514587402344 963.8735961914062 303.3811950683594]]\n",
      "[0.9940294027328491]\n",
      "828.868896484375 146.41514587402344 963.8735961914062 303.3811950683594\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE440>\n",
      "[[827.7084350585938 145.6385955810547 962.4047241210938 303.8179931640625]]\n",
      "[0.9984295964241028]\n",
      "827.7084350585938 145.6385955810547 962.4047241210938 303.8179931640625\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDBD0>\n",
      "[[831.537841796875 147.1241455078125 966.63037109375 311.1765441894531]]\n",
      "[0.9870145320892334]\n",
      "831.537841796875 147.1241455078125 966.63037109375 311.1765441894531\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDC30>\n",
      "[[828.901123046875 146.9303436279297 963.7238159179688 312.5634765625]]\n",
      "[0.9953020811080933]\n",
      "828.901123046875 146.9303436279297 963.7238159179688 312.5634765625\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CDB40>\n",
      "[[830.6724243164062 148.79586791992188 957.4552612304688\n",
      "  300.1648254394531]]\n",
      "[0.9943337440490723]\n",
      "830.6724243164062 148.79586791992188 957.4552612304688 300.1648254394531\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE4A0>\n",
      "[[821.5454711914062 144.37379455566406 965.3416748046875 327.677001953125]]\n",
      "[0.9939641356468201]\n",
      "821.5454711914062 144.37379455566406 965.3416748046875 327.677001953125\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE5F0>\n",
      "[[821.8457641601562 146.29217529296875 965.2803955078125\n",
      "  326.6094970703125]\n",
      " [1147.0968017578125 331.1733093261719 1185.9656982421875\n",
      "  386.0328369140625]]\n",
      "[0.9924944639205933 0.7278359532356262]\n",
      "821.8457641601562 146.29217529296875 965.2803955078125 326.6094970703125\n",
      "1147.0968017578125 331.1733093261719 1185.9656982421875 386.0328369140625\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE5C0>\n",
      "[[827.3565673828125 147.90350341796875 954.1124267578125\n",
      "  299.5915222167969]]\n",
      "[0.99310302734375]\n",
      "827.3565673828125 147.90350341796875 954.1124267578125 299.5915222167969\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE560>\n",
      "[[820.4989624023438 144.12294006347656 963.8244018554688\n",
      "  326.4127502441406]]\n",
      "[0.9935981035232544]\n",
      "820.4989624023438 144.12294006347656 963.8244018554688 326.4127502441406\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE680>\n",
      "[[820.7867431640625 145.13172912597656 960.4098510742188\n",
      "  325.5835266113281]]\n",
      "[0.9986588954925537]\n",
      "820.7867431640625 145.13172912597656 960.4098510742188 325.5835266113281\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE710>\n",
      "[[817.9657592773438 143.95570373535156 960.4306030273438\n",
      "  326.4449768066406]]\n",
      "[0.9975511431694031]\n",
      "817.9657592773438 143.95570373535156 960.4306030273438 326.4449768066406\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE770>\n",
      "[[817.7931518554688 142.00241088867188 960.7947998046875\n",
      "  325.0296630859375]]\n",
      "[0.9919448494911194]\n",
      "817.7931518554688 142.00241088867188 960.7947998046875 325.0296630859375\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE6E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1699 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[818.9473876953125 146.16432189941406 958.5640869140625\n",
      "  324.52606201171875]]\n",
      "[0.9973716735839844]\n",
      "818.9473876953125 146.16432189941406 958.5640869140625 324.52606201171875\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE7A0>\n",
      "[[816.6475830078125 146.7167205810547 955.87744140625 323.7806701660156]]\n",
      "[0.9993502497673035]\n",
      "816.6475830078125 146.7167205810547 955.87744140625 323.7806701660156\n",
      "<PIL.Image.Image image mode=RGB size=1920x1080 at 0x28A138CE650>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\Master\\Research Project\\Code\\computer-vision\\preprocessing.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m create_face_images(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     video_files, \u001b[39m\"\u001b[39;49m\u001b[39mF:/Master/Research Project/Generated Data/DFDC_Face_only_data\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "\u001b[1;32mf:\\Master\\Research Project\\Code\\computer-vision\\preprocessing.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m idx \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m150\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Detect faces using MTCNN\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     boxes, probs \u001b[39m=\u001b[39m mtcnn\u001b[39m.\u001b[39;49mdetect(frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(boxes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Master/Research%20Project/Code/computer-vision/preprocessing.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(probs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[0;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[0;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[0;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[0;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:71\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m scale \u001b[39min\u001b[39;00m scales:\n\u001b[1;32m---> 71\u001b[0m     im_data \u001b[39m=\u001b[39m imresample(imgs, (\u001b[39mint\u001b[39;49m(h \u001b[39m*\u001b[39;49m scale \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39mint\u001b[39;49m(w \u001b[39m*\u001b[39;49m scale \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)))\n\u001b[0;32m     72\u001b[0m     im_data \u001b[39m=\u001b[39m (im_data \u001b[39m-\u001b[39m \u001b[39m127.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.0078125\u001b[39m\n\u001b[0;32m     73\u001b[0m     reg, probs \u001b[39m=\u001b[39m pnet(im_data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:305\u001b[0m, in \u001b[0;36mimresample\u001b[1;34m(img, sz)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimresample\u001b[39m(img, sz):\n\u001b[1;32m--> 305\u001b[0m     im_data \u001b[39m=\u001b[39m interpolate(img, size\u001b[39m=\u001b[39;49msz, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39marea\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m im_data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:3999\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3997\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marea\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   3998\u001b[0m     \u001b[39massert\u001b[39;00m output_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 3999\u001b[0m     \u001b[39mreturn\u001b[39;00m adaptive_avg_pool2d(\u001b[39minput\u001b[39;49m, output_size)\n\u001b[0;32m   4000\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marea\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   4001\u001b[0m     \u001b[39massert\u001b[39;00m output_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:1228\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size)\n\u001b[0;32m   1227\u001b[0m _output_size \u001b[39m=\u001b[39m _list_with_default(output_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 1228\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49madaptive_avg_pool2d(\u001b[39minput\u001b[39;49m, _output_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_face_images(\n",
    "    video_files, \"F:/Master/Research Project/Generated Data/DFDC_Face_only_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding(path):\n",
    "    img = Image.open(path)\n",
    "    # Get cropped and prewhitened image tensor\n",
    "    img_cropped = mtcnn(img)\n",
    "    # Calculate embedding (unsqueeze to add batch dimension)\n",
    "    img_embedding = resnet(img_cropped)\n",
    "    print(img_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0185,  0.0610, -0.0175,  0.0180, -0.0069, -0.0006, -0.0789,  0.0291,\n",
      "          0.0499,  0.0151, -0.0404,  0.0242, -0.0472, -0.0576,  0.0560,  0.0413,\n",
      "          0.0925,  0.0263,  0.0467, -0.0192,  0.0329,  0.0220,  0.0178,  0.0354,\n",
      "         -0.0150,  0.0540,  0.0393,  0.0331,  0.0079,  0.0522, -0.0300,  0.0391,\n",
      "          0.0026,  0.0156, -0.0275,  0.0024,  0.0091,  0.0028, -0.0062,  0.0398,\n",
      "          0.0659, -0.0257, -0.0253,  0.0576, -0.0578, -0.0390, -0.0011,  0.0350,\n",
      "         -0.1192, -0.0072, -0.0280,  0.0353,  0.0183, -0.0460, -0.0412,  0.0268,\n",
      "          0.0168,  0.0198,  0.0812,  0.0049, -0.0295,  0.0719, -0.0520, -0.0298,\n",
      "         -0.0192,  0.0465, -0.0334,  0.0291, -0.0380, -0.0025,  0.0680, -0.0478,\n",
      "          0.0102,  0.0113,  0.0242,  0.0213, -0.1114,  0.0629, -0.0384, -0.0510,\n",
      "          0.0802, -0.0413, -0.0689,  0.0344, -0.0203, -0.0769, -0.0447,  0.1119,\n",
      "         -0.0433, -0.0188, -0.0376,  0.0331, -0.0069,  0.0328,  0.0414, -0.0675,\n",
      "         -0.0619, -0.0006, -0.0911,  0.0015,  0.0246,  0.0305, -0.0934, -0.0065,\n",
      "          0.0684,  0.0212,  0.0679, -0.0499,  0.0358,  0.0292, -0.0016, -0.0461,\n",
      "         -0.1018,  0.0048,  0.0259,  0.0324,  0.0043,  0.0209, -0.0281,  0.0168,\n",
      "          0.0100, -0.0008,  0.0035, -0.0463, -0.0470, -0.0377,  0.0463,  0.0257,\n",
      "          0.0058,  0.0013, -0.0641, -0.0123,  0.0599, -0.1015, -0.0360,  0.0610,\n",
      "          0.0013, -0.0535,  0.0056, -0.0065,  0.0101, -0.0674, -0.0061,  0.0251,\n",
      "          0.0336, -0.0885,  0.0116, -0.0211,  0.0528, -0.0449,  0.0206,  0.0012,\n",
      "          0.0570, -0.0253,  0.0668,  0.0093, -0.0344,  0.0012,  0.0280, -0.0357,\n",
      "          0.0500,  0.0512, -0.0195,  0.0975,  0.0607, -0.0078, -0.0528,  0.0320,\n",
      "         -0.0137,  0.0207,  0.0012, -0.0341, -0.0545,  0.0449, -0.0062,  0.0123,\n",
      "          0.0363, -0.0149,  0.0100, -0.0276, -0.0591, -0.0044,  0.0040,  0.0059,\n",
      "          0.0003, -0.0073,  0.0073,  0.0876,  0.0108,  0.0128,  0.0788, -0.0050,\n",
      "          0.0647, -0.0159,  0.1047, -0.0432, -0.0050,  0.0148,  0.0204,  0.0571,\n",
      "          0.0563, -0.0395,  0.0723,  0.0005, -0.0083,  0.0310,  0.0601, -0.0033,\n",
      "          0.0030,  0.0087, -0.0117,  0.0442, -0.0517,  0.0776,  0.0147, -0.0667,\n",
      "          0.0372,  0.0134, -0.0312, -0.0031, -0.0348, -0.0109, -0.0174, -0.0290,\n",
      "          0.0155, -0.0548, -0.0289,  0.0164, -0.0469,  0.0652, -0.0610, -0.0590,\n",
      "          0.0305, -0.0619, -0.0183,  0.0167,  0.0543,  0.0264, -0.0666, -0.0442,\n",
      "         -0.0214, -0.0033,  0.0849,  0.0427,  0.0420,  0.1344,  0.0082,  0.0170,\n",
      "          0.0133,  0.0026, -0.0034,  0.0208,  0.0447,  0.0015, -0.0849,  0.0038,\n",
      "          0.0330,  0.0416,  0.0359,  0.0121, -0.0369, -0.0584, -0.0402, -0.0113,\n",
      "         -0.0345, -0.0379,  0.0512, -0.0387, -0.0666,  0.0400,  0.0071, -0.0721,\n",
      "         -0.0066,  0.0071, -0.0474,  0.0146, -0.0958, -0.0062, -0.0214, -0.0050,\n",
      "         -0.0045,  0.0215,  0.0180, -0.0312, -0.0275,  0.0584, -0.0050, -0.0647,\n",
      "          0.0517, -0.0195,  0.0084,  0.0050,  0.0045,  0.0015, -0.0488, -0.0968,\n",
      "          0.1234, -0.0138,  0.0385,  0.0492, -0.0003, -0.0612, -0.0913,  0.0219,\n",
      "         -0.0418,  0.0038, -0.0052, -0.0237,  0.0295, -0.0716,  0.0314, -0.0117,\n",
      "         -0.0479,  0.0051, -0.0207, -0.0250,  0.0669, -0.0196,  0.1119, -0.1038,\n",
      "         -0.0065, -0.0442, -0.0882,  0.0618, -0.0573,  0.0611,  0.0854,  0.0904,\n",
      "          0.0249,  0.0679,  0.0025, -0.0157, -0.0631, -0.0330,  0.0037,  0.0232,\n",
      "         -0.0004, -0.0906,  0.0347, -0.0215, -0.0603, -0.0273,  0.0036,  0.0207,\n",
      "         -0.0128, -0.0677,  0.0383,  0.0478, -0.0392,  0.0780,  0.0256,  0.1046,\n",
      "         -0.1067,  0.0304, -0.0602,  0.0233,  0.0271,  0.0329,  0.0304,  0.0010,\n",
      "          0.0101,  0.0199, -0.0473, -0.0461, -0.0196,  0.1232,  0.0170,  0.0265,\n",
      "          0.0394, -0.0060, -0.0766, -0.0285,  0.0240, -0.0043,  0.1076,  0.0054,\n",
      "          0.0003, -0.0109,  0.0287, -0.0417,  0.0103, -0.0239,  0.0013,  0.0690,\n",
      "          0.0313, -0.0012,  0.0073,  0.0089,  0.0311, -0.0770,  0.0258,  0.0048,\n",
      "         -0.0132, -0.0710, -0.0287, -0.0423, -0.0466, -0.0065, -0.0402,  0.0013,\n",
      "          0.0278,  0.0080,  0.0551,  0.0107, -0.0208, -0.0428,  0.0513,  0.0110,\n",
      "          0.0921,  0.0339,  0.1002, -0.0052,  0.0352,  0.0225, -0.0600,  0.0452,\n",
      "         -0.0224, -0.0573, -0.0027, -0.0271,  0.0183,  0.0020,  0.0078,  0.0076,\n",
      "         -0.0889, -0.0692, -0.0172, -0.0360,  0.0005,  0.0230, -0.0075, -0.0100,\n",
      "         -0.0150, -0.0174,  0.0082,  0.0265, -0.0218, -0.0210,  0.0379, -0.0106,\n",
      "         -0.0008,  0.0559,  0.0068, -0.0171, -0.0013, -0.0185, -0.0654,  0.0165,\n",
      "         -0.0477,  0.0304, -0.0397,  0.0147, -0.0547,  0.0271, -0.0219,  0.0027,\n",
      "         -0.0417,  0.0749,  0.0247,  0.0801,  0.0731, -0.0032,  0.0056, -0.0431,\n",
      "          0.0393,  0.0155, -0.0809, -0.0219,  0.0140,  0.0410,  0.0274, -0.0423,\n",
      "          0.0511, -0.0774, -0.0197,  0.0107,  0.0325,  0.0796, -0.0247,  0.0132,\n",
      "          0.0635, -0.0724, -0.0120,  0.0155,  0.0489,  0.0006, -0.0568, -0.0032,\n",
      "          0.0245, -0.0415,  0.0353, -0.0905, -0.0383, -0.0037, -0.0492, -0.0365,\n",
      "          0.0036, -0.0120, -0.0081, -0.0310, -0.0763,  0.0600,  0.0039,  0.0890,\n",
      "         -0.0525, -0.0786,  0.0077, -0.0084, -0.0425,  0.0289,  0.0260, -0.0165]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "calculate_embedding(\"F:\\Master\\Research Project\\Generated Data\\DFDC_Face_only_data\\\\aassnaulhq_0_face_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = display.display(frames_tracked[0], display_id=True)\n",
    "i = 1\n",
    "try:\n",
    "    while True:\n",
    "        d.update(frames_tracked[i % len(frames_tracked)])\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
